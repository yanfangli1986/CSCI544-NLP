2019-10-23 00:48:22.853 INFO:	loading vocab from tmp/vocab.txt
2019-10-23 00:48:22.858 INFO:	Vocab has 10002 types
2019-10-23 00:48:22.858 INFO:	loading data from tmp/train.txt
2019-10-23 00:48:24.330 INFO:	Found 144526 records in tmp/train.txt
2019-10-23 00:48:24.398 INFO:	loading data from tmp/dev.txt
2019-10-23 00:48:24.763 INFO:	Found 36131 records in tmp/dev.txt
/anaconda3/envs/nlp_course/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
2019-10-23 00:48:24.790 INFO:	Moving model to cpu
2019-10-23 00:48:24.791 INFO:	Device for training cpu
0it [00:00, ?it/s]                  Traceback (most recent call last):
  File "/Users/chaoyanghe/USC/CS544-NLP and its applications/CSCI-544/544_nlp_course/train.py", line 171, in <module>
    main()
  File "/Users/chaoyanghe/USC/CS544-NLP and its applications/CSCI-544/544_nlp_course/train.py", line 158, in main
    valid_data=dev_data)
  File "/Users/chaoyanghe/USC/CS544-NLP and its applications/CSCI-544/544_nlp_course/train.py", line 66, in train
    log_probs = model(cur_seqs)
  File "/anaconda3/envs/nlp_course/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/Users/chaoyanghe/USC/CS544-NLP and its applications/CSCI-544/544_nlp_course/BiLSTM_ATT_LM.py", line 59, in forward
    attn_out = self.attention_network(fbout, fbhn)
  File "/Users/chaoyanghe/USC/CS544-NLP and its applications/CSCI-544/544_nlp_course/BiLSTM_ATT_LM.py", line 37, in attention_network
    new_hidden = torch.bmm(encoder_out.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)
RuntimeError: Expected 3-dimensional tensor, but got 4-dimensional tensor for argument #2 'batch2' (while checking arguments for bmm)
