2019-10-23 00:28:02.879 INFO:	loading vocab from tmp/vocab.txt
2019-10-23 00:28:02.885 INFO:	Vocab has 10002 types
2019-10-23 00:28:02.886 INFO:	loading data from tmp/train.txt
2019-10-23 00:28:04.461 INFO:	Found 144526 records in tmp/train.txt
2019-10-23 00:28:04.556 INFO:	loading data from tmp/dev.txt
2019-10-23 00:28:04.938 INFO:	Found 36131 records in tmp/dev.txt
/anaconda3/envs/nlp_course/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
2019-10-23 00:28:04.968 INFO:	Moving model to cpu
2019-10-23 00:28:04.969 INFO:	Device for training cpu
0it [00:00, ?it/s]                  Traceback (most recent call last):
  File "./train.py", line 170, in <module>
    main()
  File "./train.py", line 157, in main
    valid_data=dev_data)
  File "./train.py", line 66, in train
    log_probs = model(cur_seqs)
  File "/anaconda3/envs/nlp_course/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/Users/chaoyanghe/USC/CS544-NLP and its applications/CSCI-544/544_nlp_course/BiLSTM_ATT_LM.py", line 58, in forward
    attn_out = self.attention_network(fbout, fbhn)
  File "/Users/chaoyanghe/USC/CS544-NLP and its applications/CSCI-544/544_nlp_course/BiLSTM_ATT_LM.py", line 34, in attention_network
    attn_weights = torch.bmm(encoder_out, hidden.unsqueeze(2)).squeeze(2)
RuntimeError: Expected tensor to have size 1 at dimension 0, but got size 256 for argument #2 'batch2' (while checking arguments for bmm)
